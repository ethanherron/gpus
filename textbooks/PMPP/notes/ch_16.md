# Deep Learning

## Chapter Outline
- 16.1 Background
- 16.2 Convolutional neural networks
- 16.3 Convolutional layer: a CUDA inference kernel
- 16.4 Formulating a convolutional layer as GEMM
- 16.5 CUDNN library
- 16.6 Summary

## 16.2 Convolutional Neural Networks
### Convolutional Neural Network Inference

- Input and output feature maps (or features) form the network
- Each output pixel produced by:
  1. Performing convolution between input patch and filter bank
  2. Feeding result through activation function
- Each convolutional layer uses multiple filter banks

#### Implementation Details

- Input feature maps stored in 3D array `X[C, H, W]`
  - C: Number of input feature maps (channels)
  - H: Height of each feature map
  - W: Width of each feature map
- Output feature maps stored in 3D array `Y[M, H-K+1, W-K+1]`
  - M: Number of output feature maps
  - K: Filter size
- Filter banks stored in 4D array `W[M, C, K, K]`

```c
void convLayer_forward(int M, int C, int H, int W, int K, float* X, float* W, float* Y) {
    int H_out = H - K + 1;
    int W_out = W - K + 1;
    for(int m = 0; m < M; m++) // for each output feature map
        for(int h = 0; h < H_out; h++) // for each output element
            for(int w = 0; w < W_out; w++) {
                Y[m, h, w] = 0;
                for(int c = 0; c < C; c++) // sum over all input feature maps
                    for(int p = 0; p < K; p++) // KxK filter
                        for(int q = 0; q < K; q++)
                            Y[m, h, w] += X[c, h + p, w + q] * W[m, c, p, q];
            }
}
```

### Subsampling Layers

- Reduces feature map size by combining pixels
- Each output pixel from a 2×2 neighborhood in input feature map
- Maintains same number of feature maps but halves rows and columns
- Often applies activation function after averaging

```c
void subsamplingLayer_forward(int M, int H, int W, int K, float* Y, float* S) {
    for(int m = 0; m < M; m++) // for each output feature map
        for(int h = 0; h < H/K; h++) // for each output element
            for(int w = 0; w < W/K; w++) {
                S[m, h, w] = 0.; 
                for(int p = 0; p < K; p++) // loop over KxK input samples
                    for(int q = 0; q < K; q++)
                        S[m, h, w] += Y[m, K*h + p, K*w+ q] /(K*K);
                
                // add bias and apply non-linear activation
                S[m, h, w] = sigmoid(S[m, h, w] + b[m]);
            }
}
```

### Convolutional Neural Network Backpropagation

#### Gradient Calculations

- For fully connected layer `y = w·x`:
  - `∂E/∂x = wᵀ·∂E/∂y`
  - `∂E/∂w = ∂E/∂y·xᵀ`

- For convolutional layer:
  - Gradient with respect to input: "backward convolution" with transposed filters
  - Gradient with respect to weights: accumulation over output locations

```c
void convLayer_backward_x_grad(int M, int C, int H_in, int W_in, int K, float* dE_dY, float* W, float* dE_dX) {
    int H_out = H_in - K + 1;
    int W_out = W_in - K + 1;
    // Initialize gradients to zero
    for(int c = 0; c < C; c++)
        for(int h = 0; h < H_in; h++)
            for(int w = 0; w < W_in; w++)
                dE_dX[c, h, w] = 0;
    
    // Compute gradients
    for(int m = 0; m < M; m++)
        for(int h = 0; h < H-1; h++)
            for(int w = 0; w < W-1; w++)
                for(int c = 0; c < C; c++)
                    for(int p = 0; p < K; p++)
                        for(int q = 0; q < K; q++)
                            if(h-p >= 0 && w-p >=0 && h-p < H_out && w-p < W_out)
                                dE_dX[c, h, w] += dE_dY[m, h-p, w-p] * W[m, c, k-p, k-q];
}
```

### Minibatch Processing


```c
void convLayer_batched(int N, int M, int C, int H, int W, int K, float* X, float* W, float* Y) {
    int H_out = H - K + 1;
    int W_out = W - K + 1;
    for(int n = 0; n < N; n++) // for each sample in the mini-batch
        for(int m = 0; m < M; m++) // for each output feature map
            for(int h = 0; h < H_out; h++) // for each output element
                for(int w = 0; w < W_out; w++) {
                    Y[n, m, h, w] = 0;
                    for (int c = 0; c < C; c++) // sum over all input feature maps
                        for (int p = 0; p < K; p++) // KxK filter
                            for (int q = 0; q < K; q++)
                                Y[n,m,h,w] += X[n, c, h+p, w+q]*W[m,c,p,q];
                }
}
```

## 16.3 Convolutional Layer: CUDA Implementation

### Parallelism Opportunities

- Four levels of "easy" parallelism:
  1. Samples in minibatch (n)
  2. Output feature maps (m)
  3. Output rows (h)
  4. Output columns (w)
- Total potential parallelism: N×M×H_out×W_out

### Thread Organization

- 2D thread blocks: Each thread computes one output pixel
- Each block computes a tile of TILE_WIDTH×TILE_WIDTH pixels
- 3D grid organization:
  1. X dimension: Output feature maps (M)
  2. Y dimension: Location of tile within output (linearized)
  3. Z dimension: Samples in minibatch (N)

```c
#define TILE_WIDTH 16
W_grid = W_out/TILE_WIDTH; // number of horizontal tiles per output map
H_grid = H_out/TILE_WIDTH; // number of vertical tiles per output map
T = H_grid * W_grid;
dim3 blockDim(TILE_WIDTH, TILE_WIDTH, 1);
dim3 gridDim(M, T, N);
ConvLayerForward_Kernel<<< gridDim, blockDim>>>(/*...*/);
```

### CUDA Kernel Implementation

```c
__global__ void ConvLayerForward_Kernel(int C, int W_grid, int K, float* X, float* W, float* Y) {
    int m = blockIdx.x;
    int h = (blockIdx.y / W_grid)*TILE_WIDTH + threadIdx.y;
    int w = (blockIdx.y % W_grid)*TILE_WIDTH + threadIdx.x;
    int n = blockIdx.z;
    float acc = 0.;
    for (int c = 0; c < C; c++) { // sum over all input channels
        for (int p = 0; p < K; p++) // loop over KxK filter
            for (int q = 0; q < K; q++)
                acc += X[n, c, h + p, w + q] * W[m, c, p, q];
    }
    Y[n, m, h, w] = acc;
}
```

- Performance limited by global memory bandwidth
- Can be optimized using constant memory caching and shared memory tiling

## 16.4 Formulating a Convolutional Layer as GEMM

### Matrix Multiplication Approach

- Represent convolution as a matrix multiplication operation
- Key idea: Unfold input feature maps to align with filter operations
- Allows using optimized GEMM (General Matrix Multiply) kernels

### Implementation Steps

1. Rearrange input pixels into expanded matrix X_unrolled
   - Each column contains all inputs needed for one output element
   - Input features are concatenated vertically

2. Arrange filter banks as a filter matrix
   - Each row contains weights for one output feature map

3. Perform matrix multiplication: Y = W × X_unrolled
   - Result is output feature maps in linearized form

![GEMM conversion](https://example.com/gemm_conv.png)

### Memory Considerations

- Input patches overlap, leading to data duplication
- Expansion ratio for input typically approaches K²
- Filter banks don't require duplication

### Implementation

```c
void unroll(int C, int H, int W, int K, float* X, float* X_unroll) {
    int H_out = H - K + 1;
    int W_out = W - K + 1;
    for(int c = 0; c < C; c++) {
        // Beginning row index for channel c
        w_base = c * (K*K);
        for(int p = 0; p < K; p++) {
            for(int q = 0; q < K; q++) {
                for(int h = 0; h < H_out; h++) {
                    int h_unroll = w_base + p*K + q;
                    for(int w = 0; w < W_out; w++) {
                        int w_unroll = h * W_out + w;
                        X_unroll[h_unroll, w_unroll) = X(c, h + p, w + q);
                    }
                }
            }
        }
    }
}
```

### CUDA Implementation

```c
__global__ void unroll_Kernel(int C, int H, int W, int K, float* X, float* X_unroll) {
    int t = blockIdx.x * blockDim.x + threadIdx.x;
    int H_out = H - K + 1;
    int W_out = W - K + 1;
    int W_unroll = H_out * W_out;
    
    if (t < C * W_unroll) {
        int c = t / W_unroll;
        int w_unroll = t % W_unroll;
        int h_out = w_unroll / W_out;
        int w_out = w_unroll % W_out;
        int w_base = c * K * K;
        
        for(int p = 0; p < K; p++)
            for(int q = 0; q < K; q++) {
                int h_unroll = w_base + p*K + q;
                X_unroll[h_unroll, w_unroll] = X[c, h_out + p, w_out + q];
            }
    }
}
```

### Advantages and Disadvantages

#### Advantages:
- Uses highly optimized matrix multiplication routines
- Consistently high performance across different layer configurations
- Efficient for both early layers (small C, large H/W) and late layers (large C, small H/W)

#### Disadvantages:
- Memory expansion (up to K² times)
- Increased memory traffic from writing/reading X_unrolled
- May limit parallelism when processing minibatch iteratively

## 16.5 CUDNN Library

- Optimized library for deep learning primitives
- Designed for easy integration with deep learning frameworks
- Requires data to be resident in GPU memory

### Convolution Parameters

- N: Number of images in minibatch
- C: Number of input feature maps
- H/W: Height/width of input
- K: Number of output feature maps
- R/S: Height/width of filter
- u/v: Vertical/horizontal stride
- pad_h/pad_w: Height/width of zero padding

### CUDNN Approach

- Supports multiple convolution algorithms:
  - GEMM-based (matrix multiplication)
  - Winograd algorithm
  - FFT-based algorithms

- Avoids materializing X_unrolled in global memory
  - Generates and loads X_unrolled into on-chip memory only
  - Handles indexing complexities during tile management
  - Hides memory latency with computation

- Performs optimized memory operations:
  - Dynamically computes mapping between tiles and convolution
  - Leverages optimized matrix multiplication infrastructure
  - Performs tensor transposition for desired data layout

## 16.6 Summary

- GPU implementations can leverage:
  - Direct convolutional implementations with CUDA
  - Matrix multiplication formulations (GEMM)
  - Optimized libraries like CUDNN