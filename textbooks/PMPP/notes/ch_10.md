# 10 Reduction

## Chapter Outline
- 10.1 Background
- 10.2 Reduction trees
- 10.3 A simple reduction kernel
- 10.4 Minimizing control divergence
- 10.5 Minimizing memory divergence
- 10.6 Minimizing global memory accesses
- 10.7 Hierarchical reduction for arbitrary input length
- 10.8 Thread coarsening for reduced overhead
- 10.9 Summary

## Introduction

Reduction is a fundamental parallel pattern that derives a single value from an array of values. Common reduction operations include:
- Sum (adding all elements)
- Maximum (finding the largest value)
- Minimum (finding the smallest value)

Like histograms, reduction generates a summary from large datasets, but requires different coordination strategies among parallel threads. Reduction algorithms highlight important performance bottlenecks in parallel computing and demonstrate techniques for mitigating them.

This chapter explores:
- Mathematical foundations of reduction operations
- Parallel reduction algorithms and tree structures
- Implementation techniques for efficient GPU kernels
- Strategies for minimizing thread divergence

## 10.1 Background

A reduction operation applies a binary operator to a set of values to produce a single result.

### Requirements for Reduction Operations

For a binary operator to be used in reduction, it must have:
- A well-defined identity value
- Mathematical properties that allow parallel execution

Examples of reduction operators and their identity values:
- Addition: identity value = 0.0 (v + 0.0 = v)
- Multiplication: identity value = 1.0 (v × 1.0 = v)
- Minimum: identity value = +∞ (min(v, +∞) = v)
- Maximum: identity value = -∞ (max(v, -∞) = v)

### Sequential Reduction

The sequential algorithm for sum reduction is straightforward:

```c
float sum_reduction(float* input, int length) {
    float sum = 0.0f;  // Initialize with identity value
    for (int i = 0; i < length; i++) {
        sum += input[i];
    }
    return sum;
}
```

This sequential algorithm:
- Has O(N) time complexity
- Processes one element at a time
- Accumulates the result in a single variable

### General Reduction Form

For any reduction operator:

```c
result_type reduction(input_type* input, int length) {
    result_type acc = IDENTITY_VALUE;
    for (int i = 0; i < length; i++) {
        acc = Operator(acc, input[i]);
    }
    return acc;
}
```

Where `Operator` is the binary function (add, multiply, min, max, etc.) used for the reduction.

## 10.2 Reduction trees

Parallel reduction algorithms employ a tree structure to efficiently combine values in logarithmic time.

### Parallel Reduction Concept

The key idea is to:
- Divide the input array into pairs
- Apply the operator to each pair in parallel
- Recursively combine the partial results until a single value remains

For example, with a max reduction on 8 elements:
1. First step: 4 parallel operations on pairs of elements
2. Second step: 2 parallel operations on the 4 results from step 1
3. Third step: 1 operation on the 2 results from step 2 to produce the final result

### Mathematical Properties for Parallelization

For parallel reduction to work correctly, the operator must be:
- **Associative**: (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c)
  - Allows rearranging of parentheses in expressions
  - Enables different execution orders in parallel algorithms
  - Examples: addition, multiplication, min, max

- **Commutative**: a ⊕ b = b ⊕ a (required for some optimizations)
  - Allows rearranging operand order
  - Examples: addition, multiplication, min, max

Note: Floating-point operations are not strictly associative due to rounding errors, but most applications tolerate small differences in results.

### Efficiency of Reduction Trees

For N input elements, a reduction tree:
- Takes log₂N time steps (compared to N steps for sequential)
- Performs a total of N-1 operations (same as sequential)
- Requires up to N/2 execution units for maximum parallelism
- Has peak parallelism of N/2 (first step) and average parallelism of (N-1)/log₂N

For N=1024:
- Sequential: 1024 time steps
- Parallel: 10 time steps
- Theoretical speedup: 102.4×
- Peak parallelism: 512 execution units (first step)
- Average parallelism: ~102.3 execution units

This variation in parallelism across time steps makes reduction a challenging pattern for parallel computing systems.

## 10.3 A simple reduction kernel

A basic CUDA reduction kernel implements the reduction tree pattern within a thread block, where all threads can synchronize with each other.

### Implementation Approach

For an input array of N elements:
- Launch a single block with N/2 threads
- Each thread processes two elements initially
- In each step, active threads combine their values with values from other threads
- The number of active threads decreases by half in each step
- After log₂N steps, thread 0 contains the final result

```cuda
__global__ void SimpleSumReductionKernel(float* input, float* output) {
    int i = 2 * threadIdx.x;
    
    for (int stride = 1; stride <= blockDim.x; stride *= 2) {
        if (threadIdx.x % stride == 0) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }
    
    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
```

### Execution Pattern

In this implementation:
- Each thread "owns" a location in the input array (position 2 × threadIdx.x)
- Only the owner thread can update its location (owner-computes rule)
- In each iteration, the stride doubles (1, 2, 4, 8, ...)
- Each active thread adds a value from an element that is stride away from its owned location
- After synchronization, the next iteration begins with fewer active threads
- Finally, thread 0 writes the result to the output

The pattern of ownership and activity creates a computation tree where:
- First iteration: Threads compute pairwise sums of adjacent elements
- Second iteration: Active threads combine pairs of pairs
- Final iteration: Thread 0 combines the last two partial sums

The `__syncthreads()` ensures all partial sums are written before the next iteration begins.

## 10.4 Minimizing control divergence

The simple reduction kernel works correctly but suffers from control divergence, which reduces execution efficiency.

### Control Divergence Problem

In the simple kernel:
- The `if (threadIdx.x % stride == 0)` condition creates thread divergence
- As iterations progress, fewer threads remain active
- In later iterations, most threads in a warp are inactive but still consume execution resources
- For a 256-element input array, execution resource utilization is only ~35%

The issue stems from increasing distance between active threads as iterations progress, causing threads within the same warp to follow different execution paths.

### Improved Thread Assignment

A better approach is to keep active threads close together:
- Assign threads to the first half of the array
- Initialize stride to block size and reduce by half in each iteration
- In each iteration, only threads with indices less than stride remain active
- Active threads have consecutive indices, minimizing divergence within warps

```cuda
__global__ void BetterSumReductionKernel(float* input, float* output) {
    int i = threadIdx.x;
    
    for (int stride = blockDim.x/2; stride > 0; stride /= 2) {
        if (threadIdx.x < stride) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }
    
    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
```

### Execution Efficiency Improvement

With this approach:
- In early iterations, entire warps are either fully active or inactive (no divergence)
- Divergence only occurs in later iterations when fewer than 32 threads remain active
- For a 256-element input array, execution resource utilization improves to ~66%
- Performance nearly doubles with minimal code changes

This optimization requires understanding how SIMD hardware executes warps and how thread divergence impacts performance.

