# 10 Reduction

## Chapter Outline
- 10.1 Background
- 10.2 Reduction trees
- 10.3 A simple reduction kernel
- 10.4 Minimizing control divergence
- 10.5 Minimizing memory divergence
- 10.6 Minimizing global memory accesses
- 10.7 Hierarchical reduction for arbitrary input length
- 10.8 Thread coarsening for reduced overhead
- 10.9 Summary

## Introduction

Reduction is a fundamental parallel pattern that derives a single value from an array of values. Common reduction operations include:
- Sum (adding all elements)
- Maximum (finding the largest value)
- Minimum (finding the smallest value)

Like histograms, reduction generates a summary from large datasets, but requires different coordination strategies among parallel threads. Reduction algorithms highlight important performance bottlenecks in parallel computing and demonstrate techniques for mitigating them.

This chapter explores:
- Mathematical foundations of reduction operations
- Parallel reduction algorithms and tree structures
- Implementation techniques for efficient GPU kernels
- Strategies for minimizing thread divergence

## 10.1 Background

A reduction operation applies a binary operator to a set of values to produce a single result.

### Requirements for Reduction Operations

For a binary operator to be used in reduction, it must have:
- A well-defined identity value
- Mathematical properties that allow parallel execution

Examples of reduction operators and their identity values:
- Addition: identity value = 0.0 (v + 0.0 = v)
- Multiplication: identity value = 1.0 (v × 1.0 = v)
- Minimum: identity value = +∞ (min(v, +∞) = v)
- Maximum: identity value = -∞ (max(v, -∞) = v)

### Sequential Reduction

The sequential algorithm for sum reduction is straightforward:

```c
float sum_reduction(float* input, int length) {
    float sum = 0.0f;  // Initialize with identity value
    for (int i = 0; i < length; i++) {
        sum += input[i];
    }
    return sum;
}
```

This sequential algorithm:
- Has O(N) time complexity
- Processes one element at a time
- Accumulates the result in a single variable

### General Reduction Form

For any reduction operator:

```c
result_type reduction(input_type* input, int length) {
    result_type acc = IDENTITY_VALUE;
    for (int i = 0; i < length; i++) {
        acc = Operator(acc, input[i]);
    }
    return acc;
}
```

Where `Operator` is the binary function (add, multiply, min, max, etc.) used for the reduction.

## 10.2 Reduction trees

Parallel reduction algorithms employ a tree structure to efficiently combine values in logarithmic time.

### Parallel Reduction Concept

The key idea is to:
- Divide the input array into pairs
- Apply the operator to each pair in parallel
- Recursively combine the partial results until a single value remains

For example, with a max reduction on 8 elements:
1. First step: 4 parallel operations on pairs of elements
2. Second step: 2 parallel operations on the 4 results from step 1
3. Third step: 1 operation on the 2 results from step 2 to produce the final result

### Mathematical Properties for Parallelization

For parallel reduction to work correctly, the operator must be:
- **Associative**: (a ⊕ b) ⊕ c = a ⊕ (b ⊕ c)
  - Allows rearranging of parentheses in expressions
  - Enables different execution orders in parallel algorithms
  - Examples: addition, multiplication, min, max

- **Commutative**: a ⊕ b = b ⊕ a (required for some optimizations)
  - Allows rearranging operand order
  - Examples: addition, multiplication, min, max

Note: Floating-point operations are not strictly associative due to rounding errors, but most applications tolerate small differences in results.

### Efficiency of Reduction Trees

For N input elements, a reduction tree:
- Takes log₂N time steps (compared to N steps for sequential)
- Performs a total of N-1 operations (same as sequential)
- Requires up to N/2 execution units for maximum parallelism
- Has peak parallelism of N/2 (first step) and average parallelism of (N-1)/log₂N

For N=1024:
- Sequential: 1024 time steps
- Parallel: 10 time steps
- Theoretical speedup: 102.4×
- Peak parallelism: 512 execution units (first step)
- Average parallelism: ~102.3 execution units

This variation in parallelism across time steps makes reduction a challenging pattern for parallel computing systems.

## 10.3 A simple reduction kernel

A basic CUDA reduction kernel implements the reduction tree pattern within a thread block, where all threads can synchronize with each other.

### Implementation Approach

For an input array of N elements:
- Launch a single block with N/2 threads
- Each thread processes two elements initially
- In each step, active threads combine their values with values from other threads
- The number of active threads decreases by half in each step
- After log₂N steps, thread 0 contains the final result

```cuda
__global__ void SimpleSumReductionKernel(float* input, float* output) {
    int i = 2 * threadIdx.x;
    
    for (int stride = 1; stride <= blockDim.x; stride *= 2) {
        if (threadIdx.x % stride == 0) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }
    
    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
```

### Execution Pattern

In this implementation:
- Each thread "owns" a location in the input array (position 2 × threadIdx.x)
- Only the owner thread can update its location (owner-computes rule)
- In each iteration, the stride doubles (1, 2, 4, 8, ...)
- Each active thread adds a value from an element that is stride away from its owned location
- After synchronization, the next iteration begins with fewer active threads
- Finally, thread 0 writes the result to the output

The pattern of ownership and activity creates a computation tree where:
- First iteration: Threads compute pairwise sums of adjacent elements
- Second iteration: Active threads combine pairs of pairs
- Final iteration: Thread 0 combines the last two partial sums

The `__syncthreads()` ensures all partial sums are written before the next iteration begins.

## 10.4 Minimizing control divergence

The simple reduction kernel works correctly but suffers from control divergence, which reduces execution efficiency.

### Control Divergence Problem

In the simple kernel:
- The `if (threadIdx.x % stride == 0)` condition creates thread divergence
- As iterations progress, fewer threads remain active
- In later iterations, most threads in a warp are inactive but still consume execution resources
- For a 256-element input array, execution resource utilization is only ~35%

The issue stems from increasing distance between active threads as iterations progress, causing threads within the same warp to follow different execution paths.

### Improved Thread Assignment

A better approach is to keep active threads close together:
- Assign threads to the first half of the array
- Initialize stride to block size and reduce by half in each iteration
- In each iteration, only threads with indices less than stride remain active
- Active threads have consecutive indices, minimizing divergence within warps

```cuda
__global__ void BetterSumReductionKernel(float* input, float* output) {
    int i = threadIdx.x;
    
    for (int stride = blockDim.x/2; stride > 0; stride /= 2) {
        if (threadIdx.x < stride) {
            input[i] += input[i + stride];
        }
        __syncthreads();
    }
    
    if (threadIdx.x == 0) {
        *output = input[0];
    }
}
```

### Execution Efficiency Improvement

With this approach:
- In early iterations, entire warps are either fully active or inactive (no divergence)
- Divergence only occurs in later iterations when fewer than 32 threads remain active
- For a 256-element input array, execution resource utilization improves to ~66%
- Performance nearly doubles with minimal code changes

This optimization requires understanding how SIMD hardware executes warps and how thread divergence impacts performance.

## 10.5 Minimizing memory divergence

The simple kernel has memory divergence issues in addition to control divergence.

### Memory Coalescing Problem

- Memory coalescing is critical for performance (covered in Chapter 5)
- Adjacent threads in a warp should access adjacent memory locations
- In the simple kernel, threads own non-adjacent locations
- Each thread performs in each iteration:
  - Two global memory reads (from owned location and stride-away location)
  - One global memory write (to owned location)
- Adjacent threads access non-adjacent locations, causing poor coalescing

### Memory Access Pattern

- First iteration problem:
  - Warp threads access locations that are two elements apart
  - Two global memory requests are triggered per operation
  - Half the data returned is not used by the threads
- In later iterations:
  - Every other thread drops out
  - Memory access distance doubles each iteration
  - Memory utilization decreases by half each step

### Memory Request Analysis

- For the simple kernel, memory requests per iteration:
  - First five iterations: N/64 × 2 (all warps have multiple active threads)
  - Later iterations: Decreasing number of active warps
  - Total equation: N/64 × 2 + N/64 + N/64 × 1/2 + N/64 × 1/4 + ... + 1 × 3
  - For 256-element input: (4 × 2 + 4 + 2 + 1) × 3 = 141 requests

- For the better kernel (Fig. 10.9):
  - Adjacent threads access adjacent memory locations
  - One global memory request per operation
  - Inactive warps make no memory requests
  - Total equation: (N/64 + N/64 × 1/2 + N/64 × 1/4 + ... + 1 + 5) × 3
  - For 256-element input: ((4 + 2 + 1) + 5) × 3 = 36 requests

- Performance improvement: 141/36 ≈ 3.9× fewer global memory requests
- Since DRAM bandwidth is limited, this significantly improves execution time

## 10.6 Minimizing global memory accesses

### Scaling to Larger Inputs

- For 2048-element input:
  - Simple kernel: (32 × 2 + 32 + 16 + 8 + 4 + 2 + 1) × 3 = 1149 requests
  - Better kernel: (32 + 16 + 8 + 4 + 2 + 1 + 5) × 3 = 204 requests
  - Improvement ratio: 5.6× (even better than smaller input)
  - Reason: Inefficient execution pattern in simple kernel is magnified with larger inputs

### Using Shared Memory for Further Improvement

- Observation: Partial sums are written to global memory, then read again in next iteration
- Solution: Keep partial sums in shared memory (much faster than global memory)
- Implementation strategy:
  - Load and add two original elements before writing to shared memory
  - Start for-loop with blockDim.x/2 (first iteration done outside loop)
  - Use __syncthreads() at loop beginning to ensure synchronization
  - Perform remaining iterations using shared memory
  - Only thread 0 writes final result to output

```cuda
__global__ void SharedMemSumReductionKernel(float* input, float* output) {
    __shared__ float input_s[BLOCK_SIZE];
    unsigned int t = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Load input into shared memory
    input_s[t] = input[i] + input[i + blockDim.x];
    __syncthreads();
    
    // Reduction in shared memory
    for (unsigned int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (t < stride)
            input_s[t] += input_s[t + stride];
        __syncthreads();
    }
    
    // Write result for this block to global memory
    if (t == 0)
        output[0] = input_s[0];
}
```

### Benefits of Shared Memory Approach

- Global memory accesses reduced to:
  - Initial loading of input array (N reads)
  - Final write to output (1 write)
- Total: N+1 global memory accesses
- With coalescing: Only (N/32)+1 global memory requests
- For 256-element input: Reduced from 36 to 8+1=9 requests (4× improvement)
- Additional benefit: Original input array values are preserved

## 10.7 Hierarchical reduction for arbitrary input length

### Single Block Limitation

- Previous kernels limited to one thread block
- Reason: __syncthreads() only works within same block
- Maximum parallelism limited to 1024 threads (current hardware)
- Need different approach for large arrays (millions/billions of elements)

### Segmented Multiblock Reduction

- Strategy: Partition input array into segments
- Each segment processed by one thread block
- Each block independently executes reduction tree
- Results combined using atomic operations
- Implementation details:
  - Segment size = 2 × blockDim.x
  - Each block's starting location = segment size × blockIdx.x
  - Example with 1024 threads per block:
    - Segment size = 2048
    - Block 0 processes elements 0-2047
    - Block 1 processes elements 2048-4095
    - Block 2 processes elements 4096-6143, etc.

### Kernel Implementation

```cuda
__global__ void MultiBlockSumReductionKernel(float* input, float* output) {
    __shared__ float input_s[BLOCK_SIZE];
    unsigned int t = threadIdx.x;
    unsigned int segment = 2 * blockDim.x * blockIdx.x;
    unsigned int i = segment + t;
    
    // Load input into shared memory
    input_s[t] = input[i] + input[i + blockDim.x];
    __syncthreads();
    
    // Reduction in shared memory
    for (unsigned int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (t < stride)
            input_s[t] += input_s[t + stride];
        __syncthreads();
    }
    
    // Write result for this block to global memory using atomic add
    if (t == 0)
        atomicAdd(output, input_s[0]);
}
```

- Each thread assigned location in global array:
  - i = segment starting location + threadIdx.x
  - t = threadIdx.x (location in shared memory)
- Load data from global memory (i) to shared memory (t)
- Execute reduction tree within block using shared memory
- When complete, partial sum for segment in input_s[0]
- Thread 0 from each block uses atomicAdd to contribute result to final output
- All blocks execute independently, final sum available after kernel completion

## 10.8 Thread coarsening for reduced overhead

### Thread Utilization Problem

- Maximum parallelism approach: N/2 threads for N elements
- With 1024-thread blocks: N/2048 thread blocks
- Hardware limitation: May only execute portion of blocks in parallel
- Surplus blocks executed sequentially
- Inefficiency increases in later stages of reduction:
  - More warps become idle
  - Control divergence increases in final warp

### Thread Coarsening Concept

- Thread coarsening: Serialize work into fewer threads to reduce overhead
- Implementation approach:
  - Assign more elements to each thread block
  - Each thread processes more elements independently
  - Example: 
    - Original: 16 elements per block (2 per thread)
    - Coarsened: 32 elements per block (4 per thread)
  - Each thread adds its elements before participating in reduction tree

### Kernel Implementation

```cuda
__global__ void CoarsenedSumReductionKernel(float* input, float* output) {
    __shared__ float input_s[BLOCK_SIZE];
    unsigned int t = threadIdx.x;
    unsigned int segment = 2 * blockDim.x * COARSE_FACTOR * blockIdx.x;
    unsigned int i = segment + t;
    
    // Each thread loads and adds COARSE_FACTOR pairs of elements
    float sum = 0.0f;
    for (int j = 0; j < COARSE_FACTOR; j++) {
        sum += input[i + j*2*blockDim.x] + input[i + j*2*blockDim.x + blockDim.x];
    }
    input_s[t] = sum;
    __syncthreads();
    
    // Reduction in shared memory
    for (unsigned int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (t < stride)
            input_s[t] += input_s[t + stride];
        __syncthreads();
    }
    
    // Write result for this block to global memory using atomic add
    if (t == 0)
        atomicAdd(output, input_s[0]);
}
```

- Changes from standard kernel:
  - Multiply segment size by COARSE_FACTOR
  - Add coarsening loop to process multiple elements per thread
  - All threads remain active during coarsening loop
  - No synchronization needed during independent processing
  - Accumulate to local variable before shared memory

### Performance Analysis

- Non-coarsened approach (2 blocks serialized):
  - 8 total steps across both blocks
  - 2 steps with full hardware utilization
  - 6 steps with underutilization and synchronization overhead

- Coarsened approach (1 block with factor of 2):
  - 6 total steps
  - 3 steps with full hardware utilization
  - 3 steps with underutilization and synchronization overhead
  - No synchronization needed during first 3 steps

- Benefits:
  - Reduced hardware underutilization
  - Fewer synchronization points
  - Less shared memory access overhead

### Coarsening Factor Considerations

- Theoretical possibility: Increase coarsening factor beyond 2
- Trade-off: Higher factor means less parallelism
- Risk: Too high factor may underutilize hardware
- Optimal strategy: Choose factor that ensures enough thread blocks to fully utilize hardware
- Depends on:
  - Input size
  - Device characteristics

## 10.9 Summary

- Reduction pattern is fundamental to many data-processing applications
- Simple sequential code requires several optimization techniques for GPU:
  - Thread index assignment to minimize divergence
  - Memory access patterns for coalescing
  - Shared memory usage to reduce global memory accesses
  - Segmented reduction with atomic operations for large inputs
  - Thread coarsening to reduce parallelization overhead
- Reduction is foundation for other parallel patterns like prefix-sum (Chapter 11)

## Exercises

1. For the simple reduction kernel in Fig 10.6, if the number of elements is 1024 and the warp size is 32, how many warps in the block will have divergence during the fifth iteration?
1024/2^5 = 32 and since this is naive there should be one element in each warp (32 warps in total) meaning all 32 warps have divergence. 

2. For the improved reduction kernel in Fig 10.9, if the number of elements is 1024 and the warp size is 32, how many warps will have divergence during the fifth iteration?
1024/2^5 = 32 and since we use the improved kernel where all threads computing sums are adjacent there should be one full warp at the 5th iteration meaning no warps are divergent. 
